# Writing a Time Series Database from Scratch

> 译注：原文地址：https://fabxc.org/tsdb/，由 Prometheus 核心开发者 Fabian Reinartz 写作于2017-04-20。
> 由于本文篇幅较长，就不做逐字逐句地翻译了，挑选其中重要精华部分翻译。

Prometheus 的存储层在过去已经展现出了出色的性能，单机可以摄入高达每秒一百万的样本和数百万的时序，而只需占用较小的磁盘空间。尽管目前的存储层已经工作得很好了，我还是提出了一个新设计的存储子系统来弥补现有方案的缺陷以及应对更高数量级的数据规模。

## 问题，问题，问题空间

首先，快速概述下我们要实现的东西及其引发的关键问题。我们来看下 Prometheus 目前的方案，它在哪些地方做得好，还有哪些问题是要用新的设计来解决的。

### 时序数据

我们有一个随着时间推移收集数据的系统。

```
identifier -> (t0, v0), (t1, v1), (t2, v2), (t3, v3), ....
```

每个**数据点**是**时间戳**和**值**的构成的元组。为了监控的目的，时间戳是一个整型，而值可以是任意数字。64位的浮点数可以很好地表示counter和gauge的值，所以我们将其作为值的数据类型。一系列时间戳**严格单调递增**的数据点是一个**序列**，这可以用一个**标识符（identifier）**来表示。我们用的标识符是指标名称和*标签维度*的字典。标签维度将单个指标的度量空间分割开。每个指标名加上唯一的标签集就是它自己的*时间序列*，并且有一个数值流和时序联系在一起。

这是一个典型的序列标识符集合，它是用于请求计数的指标的一部分：

```
requests_total{path="/status", method="GET", instance=”10.0.0.1:80”}
requests_total{path="/status", method="POST", instance=”10.0.0.3:80”}
requests_total{path="/", method="GET", instance=”10.0.0.2:80”}
```

我们来简化这个表示形式：指标名称也可以看作是另一个指标维度 —— `__name__`。在查询时，指标名可能会被特殊处理，但这和存储方式无关。

```
{__name__="requests_total", path="/status", method="GET", instance=”10.0.0.1:80”}
{__name__="requests_total", path="/status", method="POST", instance=”10.0.0.3:80”}
{__name__="requests_total", path="/", method="GET", instance=”10.0.0.2:80”}
```

当查询时序数据时，我们希望通过标签来选择序列。在最简单的情况中，`{__name__="requests_total"}` 选择所有属于`requests_total`指标的序列。对所有选定的序列，我们获取在给定的时间窗口的数据点。

在更复杂的查询中，我们可能想迅速选取出满足若干个**指标选择器（label selectors）**的序列，并且能够表示比相等关系更加复杂的条件。比如，不等（`method!="GET"`）或者正则表达式（`method=~"PUT|POST"`）。

这就定义了存储的数据和它是如何被召回的。

### 垂直和水平

在简化的视角中，所有的数据点可以置于一个二维的平面上。水平维度表示时间，序列标识符在垂直维度上扩展。

```
series
  ^   
  │   . . . . . . . . . . . . . . . . .   . . . . .   {__name__="request_total", method="GET"}
  │     . . . . . . . . . . . . . . . . . . . . . .   {__name__="request_total", method="POST"}
  │         . . . . . . .
  │       . . .     . . . . . . . . . . . . . . . .                  ... 
  │     . . . . . . . . . . . . . . . . .   . . . .   
  │     . . . . . . . . . .   . . . . . . . . . . .   {__name__="errors_total", method="POST"}
  │           . . .   . . . . . . . . .   . . . . .   {__name__="errors_total", method="GET"}
  │         . . . . . . . . .       . . . . .
  │       . . .     . . . . . . . . . . . . . . . .                  ... 
  │     . . . . . . . . . . . . . . . .   . . . . 
  v
    <-------------------- time --------------------->
```

Prometheus 通过周期性地抓取时间序列的当前值来获取数据点。被获取数据的实体称为**对象（target）**。因此，写模式是完全**垂直并且高并发**的，因为各个对象的样本是被独立地摄取（ingest）的。

举个例子，单个 Prometheus 实例从以万计的对象采集数据点，而每个对象暴露了成百上千的不同的时序。

在每秒采集数百万数据点的规模下，**批量写**毋庸置疑是达到性能要求所必须的。将零散的数据点写入到磁盘会非常慢。因此我们想**按顺序**写入更大的块（chunk）数据。

对于机械旋转式的磁盘这是意料之中的事实，因为它们的磁头必须一直移动到不同的扇区上。尽管SSD是以快速的随机写而著称的，实际上它们并不能修改单个的字节，而只能以4KiB或者更大的页来写入。这就意味着写入16字节的样本和写入完整的4KiB的页是一样的。这种行为就是所谓的[**写放大**](https://en.wikipedia.org/wiki/Write_amplification)的一部分，这会导致SSD的磨损 —— 所以这不仅仅是慢的问题，还会损伤硬件。可以阅读["Coding for SSDs"](http://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/)系列博客来获取更多信息。我们只需要记住：不论是旋转式磁盘还是SSD，**顺序和批量写入**都是理想的写模式。这是一个要坚持的简单原则。

查询模式和写模式区别很大。我们可以查询单个时序的单个数据点，10000个时序的单个数据点，单个时序几个星期的数据点，10000个时序几个星期的数据点，等等。在上述的二维平面上，**查询不是完全水平或者垂直的，而是两者结合的矩形形式**。

对于已知的查询，[聚合规则（Recording rules）](https://prometheus.io/docs/practices/rules/)可以缓解此问题，但这对于临时的查询并不是一个通用的解决方案。临时的查询也需要好的性能。

我们想要批量地写，但是我们得到的唯一的批次是跨越时序的数据点的垂直集合。当查询一个时序在某个时间窗口上的数据点时，不仅找到各个数据点在哪是很难的，我们还要读取磁盘上很多随机位置。单次查询可能会涉及到数百万样本，即使在最快的SSD上也会很慢。查询还会从磁盘上获取比16字节样本更多的数据。SSD会加载整页，HDD至少会读取整个扇区。不管是哪个，我们都会浪费宝贵的读吞吐量。

所以理想情况中，**相同时序的样本按顺序存储**，这样我们就可以用尽可能少的读取次数来扫描数据。我们只需要知道这个序列是从哪开始的就可以读取数据点。

在将采集数据写入到磁盘的理想模式和为了高效查询的数据布局之间，显然有着巨大的矛盾。这就是我们的TSDB要解决的**基本问题**。

#### 目前的解决方案

来看一下 Prometheus 目前的存储（我们称之为 V2）是如何解决这个问题的。

对于每个时序我们创建一个文件，文件中按顺序包含了时序样本。因为每几秒钟将单个数据点追加到所有这些文件上代价很高，我们对于每个序列把样本在内存中“攒成”1KiB大小的块（chunks），然后在块满的时候追加写到文件中。这个方法解决了大部分问题。写是批量的了，样本也按顺序存储。这还启用了非常高效的压缩格式，这种压缩格式基于样本和它前一个样本相比改变很小的性质。[Facebook Gorilla TSDB 论文](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)中描述了一个相似的基于chunk的方法，并且介绍了一种压缩格式可以把16字节的样本压缩到平均1.37字节大小。V2存储使用了多种压缩格式，包括了Gorilla的一种变式。

```
   ┌──────────┬─────────┬─────────┬─────────┬─────────┐           series A
   └──────────┴─────────┴─────────┴─────────┴─────────┘
          ┌──────────┬─────────┬─────────┬─────────┬─────────┐    series B
          └──────────┴─────────┴─────────┴─────────┴─────────┘ 
                              . . .
 ┌──────────┬─────────┬─────────┬─────────┬─────────┬─────────┐   series XYZ
 └──────────┴─────────┴─────────┴─────────┴─────────┴─────────┘ 
   chunk 1    chunk 2   chunk 3     ...
```

尽管这种基于chunk的方法很好，对于每个时序都有一个文件也带了一些问题：

* 实际上我们需要比正在采集的时间序列的数量更多的文件。在“序列搅动”部分有更多关于此的论述。几百万文件迟早会把文件系统的 [inodes](https://en.wikipedia.org/wiki/Inode) 消耗殆尽。这种情况下，我们只能通过重新格式化磁盘来恢复，而这种做法的侵入性和破坏性都过大。我们不想为了适配单个应用而格式化磁盘。

* 每秒有几千个chunk达到完整状态要被持久化，这就需要每秒数千次写磁盘。即使这可以通过将一个时序的几个完整的chunk一批写入来缓解，这反过来又增加了待持久化数据总体的内存占用。

* 将所有文件打开来读写是不可行的。因为大约99%的数据在24小时后不会被查询了。如果还会被查询，我们需要打开几千个文件，找到并将相关的数据点读入内存，然后关闭文件。由于这会导致高的查询延迟，数据块被大幅度缓存了，而这又引发了后续在“资源消耗”部分论述的问题。

* 最终，旧的数据要被删除，数据要从数百万的文件中移除掉。这意味着删除实际上是写密集型操作。而且，遍历数百万文件并分析它们的过程需要几个小时。当这个过程完成时，可能又要重新开始了。删除旧文件还会进一步导致SSD的写放大。

* 目前积累的 chunks 只存放在内存中。如果应用崩溃了，数据会丢失。为避免这一点，内存状态会周期性地检出到磁盘，这可能需要比我们能接受的数据丢失的时间窗口更长的耗时。恢复检查点可能需要几分钟，导致很长的重启时间。


现有设计中的关键点是 chunk 的概念，这一点我们肯定要保持下去。最近的 chunks 总是在内存中，这点也不错。毕竟，最近的数据是被查询最多的。

每个时序有一个文件，这一点我们需要找到替代的办法。

### 序列搅动 Series Churn

在 Prometheus 的上下文中，我们使用 *series churn* 这个术语来描述：一些时序不再活跃，即没有新的数据点，取而代之的是另一些新的活跃的时序。

举个例子，一个给定的微服务实例暴露的所有时序都有对应的“instance”标签，来标示它的来源。如果我们进行一次微服务的滚动更新，所有的实例都升到新版本，那就会发生序列搅动。在更加动态的环境中，这类事件可能每小时都在发生。集群编排系统，比如K8S，允许应用持续的自动扩缩容和频繁的滚动更新，每天可能创建成千上万的新应用实例，伴随着的是全新的时序。

```
series
  ^
  │   . . . . . .
  │   . . . . . .
  │   . . . . . .
  │               . . . . . . .
  │               . . . . . . .
  │               . . . . . . .
  │                             . . . . . .
  │                             . . . . . .
  │                                         . . . . .
  │                                         . . . . .
  │                                         . . . . .
  v
    <-------------------- time --------------------->
```

所以即使整个基础设施大致上保持规模不变，随着时间推移在数据库中的时序数量也会呈线性增长。一个 Prometheus 服务能够采集1000万时序的数据，但是如果数据需要从十亿时序中寻找的话，查询性能会大打折扣。

#### 目前的解决方案

现在的Prometheus V2存储有一个基于 **LevelDB** 的对所有存储的序列的索引。它能够查询包含一个给定的标签对的序列，但是缺少一个可扩展的方法来对不同标签选择的结果做结合。

例如，选择所有带有标签 `__name__="requests_total"` 的序列可以高效做到，但是选择所有带有标签 `instance="A" AND __name__="requests_total"` 的序列就会有扩展性问题。我们后面会说到是什么导致了这个问题，必须用什么方法来提高查询效率。

实际上，这个问题是寻找更好的存储系统的动机。Prometheus 需要一个改良的索引方法来快速搜索大量的时序。

### 资源消耗

资源消耗是尝试扩展 Prometheus 的一贯主题之一。但实际上困扰着用的户并不是绝对的资源消耗量。事实上，考虑到 Prometheus 的配置要求，它是在管理着难以置信的吞吐量。这个问题更多的是它在面对变化时的不可预测性和不稳定性。V2存储中会慢慢累积样本数据的chunks，导致内存消耗量随着时间爬升。当chunks完整时，它们会写入到磁盘并从内存中驱逐掉。最终，Prometheus的内存占用量会达到一个稳定状态。这个状态会持续到被监控环境的变化 —— 每次扩展应用或者做滚动更新时，*series churn* 会增加Prometheus的内存、CPU的占用和磁盘IO。

如果这种变化是不断发生的，那么它（译注：Prometheus的资源消耗）最终又会达到一个稳定状态，但是会比一个更加静态的环境要高很多。转换周期经常长达数小时，很难判断最大的资源使用量是多少。


每个时序有一个文件的做法也使得单次查询很容易导致 Prometheus 进程崩溃。当查询的数据没有缓存在内存中时，要打开被查询的序列的文件，包含了相关数据点的chunks被读到内存。如果数据量超过了可用内存，Prometheus 会 OOM 而突然退出。

查询完成后，加载的数据可以被释放掉，但是通常它会被缓存更长时间来更快地响应对相同数据的后续查询。后者显然是一个好的做法。


最后，我们看到了在SSD上下文中的写放大问题以及Prometheus是如何通过批量写来缓解这一问题的。尽管如此，在几个地方它仍然会导致写放大，因为太小的批次和数据没有和页边界准确对齐。实际上对于更大的 Prometheus 服务，有观察到硬件寿命的减少。对于具有高写吞吐量的数据库应用程序来说，这是相当正常的情况，但我们应该关注是否能够减轻这种情况。

## 重新开始

到目前为止，我们已经很好地了解了问题域、V2存储是如何解决它的，以及它的设计存在哪些问题。我们也看到了一些很好的概念，对此我们想要无缝适应。V2存储的相当一部分问题可以通过改进和部分重新设计来解决，但为了好玩（当然，在仔细评估了我的想法之后），我决定尝试编写整个时间序列数据库 —— 从零开始，即向文件系统写入字节。

重点关注的性能和资源占用是所选存储格式的直接结果。我们必须为数据找到正确的算法和磁盘布局，以实现性能良好的存储层。

下面我会抄捷径，直接到解决方案 —— 跳过头痛、失败的想法、无尽的草图、眼泪和沮丧。

### V3 —— 宏观设计

存储的宏观布局是什么？简单地说，在数据目录中执行 `tree` 命令展示的就是布局了。

```shell
$ tree ./data
./data
├── b-000001
│   ├── chunks
│   │   ├── 000001
│   │   ├── 000002
│   │   └── 000003
│   ├── index
│   └── meta.json
├── b-000004
│   ├── chunks
│   │   └── 000001
│   ├── index
│   └── meta.json
├── b-000005
│   ├── chunks
│   │   └── 000001
│   ├── index
│   └── meta.json
└── b-000006
    ├── meta.json
    └── wal
        ├── 000001
        ├── 000002
        └── 000003
```

在最顶层，我们有一系列编号的 blocks，用`b-`为前缀。每个 block 中有一个包含索引的文件和一个装有更多编号文件的“chunks”目录。“chunks”目录中就是许多序列数据点的原始chunks。

#### 许多小型数据库

##### mmap

#### 压缩

#### 保留

### 索引

#### 组合标签

## 基准测试

## 结论

